{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline net with no hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 18:18:37.115591: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-18 18:18:37.116522: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-18 18:18:37.118828: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-18 18:18:37.124769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734545917.136617   23663 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734545917.139375   23663 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-18 18:18:37.152244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kajm20/miniconda3/envs/rmnist/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-12-18 18:18:40.296676: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 776us/step - accuracy: 0.6703 - loss: 1.2417 - val_accuracy: 0.8760 - val_loss: 0.5117\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701us/step - accuracy: 0.8695 - loss: 0.5181 - val_accuracy: 0.8893 - val_loss: 0.4200\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809us/step - accuracy: 0.8833 - loss: 0.4380 - val_accuracy: 0.8982 - val_loss: 0.3824\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 734us/step - accuracy: 0.8893 - loss: 0.4076 - val_accuracy: 0.9023 - val_loss: 0.3617\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786us/step - accuracy: 0.8960 - loss: 0.3852 - val_accuracy: 0.9061 - val_loss: 0.3487\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step - accuracy: 0.8972 - loss: 0.3679 - val_accuracy: 0.9075 - val_loss: 0.3386\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701us/step - accuracy: 0.9023 - loss: 0.3498 - val_accuracy: 0.9094 - val_loss: 0.3305\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714us/step - accuracy: 0.9003 - loss: 0.3539 - val_accuracy: 0.9115 - val_loss: 0.3243\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712us/step - accuracy: 0.9068 - loss: 0.3367 - val_accuracy: 0.9126 - val_loss: 0.3193\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710us/step - accuracy: 0.9077 - loss: 0.3316 - val_accuracy: 0.9115 - val_loss: 0.3158\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - accuracy: 0.9014 - loss: 0.3618\n",
      "Test Accuracy: 91.41%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the input data to [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Flatten the images (28x28 -> 784)\n",
    "x_train = x_train.reshape(-1, 28 * 28)\n",
    "x_test = x_test.reshape(-1, 28 * 28)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create a model with no hidden layers\n",
    "model = Sequential([\n",
    "    Dense(10, activation='softmax', input_shape=(784,))  # Output layer with 10 neurons (one for each digit)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd',  # Stochastic Gradient Descent\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rl net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kajm20/miniconda3/envs/rmnist/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1:\n",
      "  Hidden Layers: 4\n",
      "  Neurons: [29, 407, 504, 328]\n",
      "  Accuracy: 0.9660\n",
      "  Complexity: 1268\n",
      "  Reward: 83.92\n",
      "\n",
      "Episode 2:\n",
      "  Hidden Layers: 4\n",
      "  Neurons: [183, 436, 473, 79]\n",
      "  Accuracy: 0.9706\n",
      "  Complexity: 1171\n",
      "  Reward: 85.35\n",
      "\n",
      "Episode 3:\n",
      "  Hidden Layers: 5\n",
      "  Neurons: [275, 427, 113, 449, 42]\n",
      "  Accuracy: 0.9729\n",
      "  Complexity: 1306\n",
      "  Reward: 84.23\n",
      "\n",
      "Episode 4:\n",
      "  Hidden Layers: 0\n",
      "  Neurons: []\n",
      "  Accuracy: 0.9232\n",
      "  Complexity: 0\n",
      "  Reward: 92.32\n",
      "\n",
      "Episode 5:\n",
      "  Hidden Layers: 5\n",
      "  Neurons: [438, 311, 488, 17, 202]\n",
      "  Accuracy: 0.9755\n",
      "  Complexity: 1456\n",
      "  Reward: 82.99\n",
      "\n",
      "Episode 6:\n",
      "  Hidden Layers: 1\n",
      "  Neurons: [118]\n",
      "  Accuracy: 0.9730\n",
      "  Complexity: 118\n",
      "  Reward: 96.12\n",
      "\n",
      "Episode 7:\n",
      "  Hidden Layers: 4\n",
      "  Neurons: [308, 368, 31, 49]\n",
      "  Accuracy: 0.9713\n",
      "  Complexity: 756\n",
      "  Reward: 89.57\n",
      "\n",
      "Episode 8:\n",
      "  Hidden Layers: 0\n",
      "  Neurons: []\n",
      "  Accuracy: 0.9246\n",
      "  Complexity: 0\n",
      "  Reward: 92.46\n",
      "\n",
      "Episode 9:\n",
      "  Hidden Layers: 2\n",
      "  Neurons: [308, 264]\n",
      "  Accuracy: 0.9757\n",
      "  Complexity: 572\n",
      "  Reward: 91.85\n",
      "\n",
      "Episode 10:\n",
      "  Hidden Layers: 5\n",
      "  Neurons: [307, 158, 112, 196, 442]\n",
      "  Accuracy: 0.9734\n",
      "  Complexity: 1215\n",
      "  Reward: 85.19\n",
      "\n",
      "Episode 11:\n",
      "  Hidden Layers: 2\n",
      "  Neurons: [69, 322]\n",
      "  Accuracy: 0.9749\n",
      "  Complexity: 391\n",
      "  Reward: 93.58\n",
      "\n",
      "Episode 12:\n",
      "  Hidden Layers: 4\n",
      "  Neurons: [434, 226, 179, 220]\n",
      "  Accuracy: 0.9757\n",
      "  Complexity: 1059\n",
      "  Reward: 86.98\n",
      "\n",
      "Episode 13:\n",
      "  Hidden Layers: 0\n",
      "  Neurons: []\n",
      "  Accuracy: 0.9239\n",
      "  Complexity: 0\n",
      "  Reward: 92.39\n",
      "\n",
      "Episode 14:\n",
      "  Hidden Layers: 2\n",
      "  Neurons: [337, 211]\n",
      "  Accuracy: 0.9796\n",
      "  Complexity: 548\n",
      "  Reward: 92.48\n",
      "\n",
      "Episode 15:\n",
      "  Hidden Layers: 4\n",
      "  Neurons: [128, 79, 376, 142]\n",
      "  Accuracy: 0.9648\n",
      "  Complexity: 725\n",
      "  Reward: 89.23\n",
      "\n",
      "Episode 16:\n",
      "  Hidden Layers: 4\n",
      "  Neurons: [224, 85, 216, 219]\n",
      "  Accuracy: 0.9737\n",
      "  Complexity: 744\n",
      "  Reward: 89.93\n",
      "\n",
      "Episode 17:\n",
      "  Hidden Layers: 5\n",
      "  Neurons: [507, 59, 383, 382, 389]\n",
      "  Accuracy: 0.9722\n",
      "  Complexity: 1720\n",
      "  Reward: 80.02\n",
      "\n",
      "Episode 18:\n",
      "  Hidden Layers: 1\n",
      "  Neurons: [174]\n",
      "  Accuracy: 0.9738\n",
      "  Complexity: 174\n",
      "  Reward: 95.64\n",
      "\n",
      "Episode 19:\n",
      "  Hidden Layers: 0\n",
      "  Neurons: []\n",
      "  Accuracy: 0.9239\n",
      "  Complexity: 0\n",
      "  Reward: 92.39\n",
      "\n",
      "Episode 20:\n",
      "  Hidden Layers: 2\n",
      "  Neurons: [496, 500]\n",
      "  Accuracy: 0.9769\n",
      "  Complexity: 996\n",
      "  Reward: 87.73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0  # Normalize input to [0, 1]\n",
    "x_test = x_test / 255.0\n",
    "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# RL hyperparameters\n",
    "MAX_HIDDEN_LAYERS = 5\n",
    "MAX_NEURONS = 512\n",
    "EPISODES = 20\n",
    "\n",
    "# Reward function parameters\n",
    "ACCURACY_WEIGHT = 100\n",
    "COMPLEXITY_PENALTY = 0.01\n",
    "\n",
    "# Function to create and train a model with a given architecture\n",
    "def create_and_train_model(hidden_layers):\n",
    "    # Create a model\n",
    "    model = Sequential([Flatten(input_shape=(28, 28))])  # Input layer\n",
    "    for neurons in hidden_layers:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))  # Output layer\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# RL agent to explore architectures\n",
    "for episode in range(EPISODES):\n",
    "    # Randomly decide the number of hidden layers and their neurons\n",
    "    num_hidden_layers = random.randint(0, MAX_HIDDEN_LAYERS)\n",
    "    hidden_layers = [random.randint(10, MAX_NEURONS) for _ in range(num_hidden_layers)]\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    accuracy = create_and_train_model(hidden_layers)\n",
    "\n",
    "    # Calculate the reward\n",
    "    complexity = sum(hidden_layers)\n",
    "    reward = (accuracy * ACCURACY_WEIGHT) - (complexity * COMPLEXITY_PENALTY)\n",
    "\n",
    "    # Log results\n",
    "    print(f\"Episode {episode + 1}:\")\n",
    "    print(f\"  Hidden Layers: {len(hidden_layers)}\")\n",
    "    print(f\"  Neurons: {hidden_layers}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Complexity: {complexity}\")\n",
    "    print(f\"  Reward: {reward:.2f}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
